{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sNKpdNe2oHd"
   },
   "source": [
    "# LoRA implementation on SmolLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rVQ6aM-2oHf"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:36.379615Z",
     "iopub.status.busy": "2025-04-21T15:09:36.378903Z",
     "iopub.status.idle": "2025-04-21T15:09:39.538715Z",
     "shell.execute_reply": "2025-04-21T15:09:39.538028Z",
     "shell.execute_reply.started": "2025-04-21T15:09:36.379596Z"
    },
    "id": "w831HxGy2oHf",
    "outputId": "19bcf603-0de8-4969-f602-f905f3c1541c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "# For tokenization and dataset loading\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "!pip install datasets\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHVeqdtC2oHg"
   },
   "source": [
    "#### Initializing device here for future use if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:39.540701Z",
     "iopub.status.busy": "2025-04-21T15:09:39.540474Z",
     "iopub.status.idle": "2025-04-21T15:09:39.545087Z",
     "shell.execute_reply": "2025-04-21T15:09:39.544462Z",
     "shell.execute_reply.started": "2025-04-21T15:09:39.540681Z"
    },
    "id": "eNw2T0cn2oHh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:39.546199Z",
     "iopub.status.busy": "2025-04-21T15:09:39.545999Z",
     "iopub.status.idle": "2025-04-21T15:09:39.559938Z",
     "shell.execute_reply": "2025-04-21T15:09:39.559216Z",
     "shell.execute_reply.started": "2025-04-21T15:09:39.546184Z"
    },
    "id": "SeBkf8cK2oHh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class smolConfig:\n",
    "    vocab_size = 49152\n",
    "    hidden_size = 576\n",
    "    intermediate_size = 1536\n",
    "    num_hidden_layers = 30\n",
    "    num_heads = 9\n",
    "    kv_heads = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:39.561016Z",
     "iopub.status.busy": "2025-04-21T15:09:39.560760Z",
     "iopub.status.idle": "2025-04-21T15:09:39.579436Z",
     "shell.execute_reply": "2025-04-21T15:09:39.578840Z",
     "shell.execute_reply.started": "2025-04-21T15:09:39.560996Z"
    },
    "id": "DqhBx1ZU2oHh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"\n",
    "    Helper function to rotate the left half of a tensor along its final dimension.\n",
    "    \"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"\n",
    "    Applies RoPE on the query and key tensors.\n",
    "    \"\"\"\n",
    "    cos, sin = cos.to(q.device), sin.to(q.device)\n",
    "\n",
    "    # Unsqueexzing to enable broadcasting\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "    return q_embed, k_embed\n",
    "\n",
    "class RotaryEmbedder(nn.Module):\n",
    "    def __init__(self, dim, base):\n",
    "        super().__init__()\n",
    "        # Precompute frequency for sine/cosine embeddings\n",
    "        self.freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        # Generate positions (sequence indices) for the input\n",
    "        pos = torch.arange(x.shape[-2], dtype=torch.long)\n",
    "        # Compute angles for sine and cosine embeddings\n",
    "        angles = torch.einsum(\"p,f->pf\", pos.float(), self.freq).unsqueeze(dim=0)\n",
    "        # Duplicate angles for sine and cosine embeddings\n",
    "        emb = torch.cat((angles, angles), dim=-1)\n",
    "        # Return cosine and sine components of the positional embeddings\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Model dimensions and attention configurations\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.hidden_size // self.num_heads\n",
    "        self.kv_heads = config.kv_heads  # Number of key-value heads\n",
    "        self.rope_theta = 10000.0  # Scaling factor for rotary embeddings\n",
    "\n",
    "        # Linear projections for queries, keys, values, and output\n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "\n",
    "        # Rotary embedding generator\n",
    "        self.rotary_emb = RotaryEmbedder(base=self.rope_theta, dim=self.head_dim)\n",
    "\n",
    "    def _repeat_kv(self, x, n_rep):\n",
    "        batch, num_key_value_heads, slen, head_dim = x.shape\n",
    "        # Expand the number of key-value heads by repeating them\n",
    "        x = x[:, :, None, :, :].expand(\n",
    "            batch, num_key_value_heads, n_rep, slen, head_dim\n",
    "        )\n",
    "        # Reshape to align with the expected multi-head attention format\n",
    "        return x.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask=None):\n",
    "        # Input dimensions: (batch_size, seq_len, hidden_size)\n",
    "        b, q, _ = x.size()\n",
    "\n",
    "        # Project input hidden states into queries, keys, and values\n",
    "        q_states = self.q_proj(x)\n",
    "        k_states = self.k_proj(x)\n",
    "        v_states = self.v_proj(x)\n",
    "\n",
    "        # Reshape and transpose for multi-head attention\n",
    "        q_states = q_states.view(b, q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k_states = k_states.view(b, q, self.kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v_states = v_states.view(b, q, self.kv_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute rotary positional embeddings\n",
    "        cos, sin = self.rotary_emb(q_states)\n",
    "        cos = cos.to(q_states.device)\n",
    "        sin = sin.to(q_states.device)\n",
    "        # Apply positional embeddings to queries and keys\n",
    "        q_states, k_states = apply_rotary_pos_emb(q_states, k_states, cos, sin)\n",
    "\n",
    "        # Repeat key and value tensors to match the number of query heads\n",
    "        __kv_groups = self.num_heads // self.kv_heads\n",
    "        k_states = self._repeat_kv(k_states, __kv_groups)\n",
    "        v_states = self._repeat_kv(v_states, __kv_groups)\n",
    "\n",
    "        # Compute attention scores (scaled dot-product attention)\n",
    "        attn_weights = torch.matmul(q_states, k_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Add attention mask (e.g., for causal or padding masking)\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # Normalize attention weights using softmax\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights = nn.functional.dropout(attn_weights, 0)\n",
    "\n",
    "        # Compute attention output\n",
    "        attn_output = torch.matmul(attn_weights, v_states)\n",
    "        # Reshape and transpose back to original format\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(b, q, -1)\n",
    "\n",
    "        # Project the attention output back to the hidden size\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        # Return the final attention output\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:39.581556Z",
     "iopub.status.busy": "2025-04-21T15:09:39.581155Z",
     "iopub.status.idle": "2025-04-21T15:09:39.595096Z",
     "shell.execute_reply": "2025-04-21T15:09:39.594466Z",
     "shell.execute_reply.started": "2025-04-21T15:09:39.581540Z"
    },
    "id": "9qCQtqzJ2oHh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        This is the Root Mean Square Normalisation class.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))  # Learnable scaling factor\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate variance along the last dimension (hidden size)\n",
    "        variance = x.pow(2).mean(-1, keepdim=True)\n",
    "\n",
    "        # Normalize and scale\n",
    "        x = x * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        \"\"\"\n",
    "        This is the gated MLP from the LLaMa architecture. Here we use the SiLU acitvation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.activation = nn.modules.activation.SiLU()\n",
    "\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.activation(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "\n",
    "class LlamaDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        This is the Llama decoder block.\n",
    "        \"\"\"\n",
    "        # Self Attention Module\n",
    "        self.self_attn = GroupedQueryAttention(config)\n",
    "\n",
    "        # FFN Module\n",
    "        self.mlp = MLP(hidden_size=config.hidden_size, intermediate_size=config.intermediate_size)\n",
    "\n",
    "        # Pre Attention and Post Attention normalisation\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, eps=1e-05)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=1e-05)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # Skip connection cache\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        # Pre-attention normalisation\n",
    "        x = self.input_layernorm(x)\n",
    "\n",
    "        # A causal attention mask (i.e., decoder can only look at tokens that it has generated thus far)\n",
    "        attention_mask = torch.triu(torch.full((attention_mask.shape[-1], attention_mask.shape[-1]),\n",
    "                                               fill_value=float('-inf')), diagonal=1)\n",
    "\n",
    "        attention_mask = attention_mask.to(x.device)\n",
    "\n",
    "        # Self-attention block\n",
    "        x = self.self_attn(x=x,attention_mask=attention_mask)\n",
    "        x += residual\n",
    "\n",
    "        # Skip connection cache for MLP\n",
    "        residual = x\n",
    "\n",
    "        # Pre-MLP normalisation\n",
    "        x = self.post_attention_layernorm(x)\n",
    "\n",
    "        # MLP block\n",
    "        x = self.mlp(x)\n",
    "        x += residual\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:39.596158Z",
     "iopub.status.busy": "2025-04-21T15:09:39.595925Z",
     "iopub.status.idle": "2025-04-21T15:09:39.610223Z",
     "shell.execute_reply": "2025-04-21T15:09:39.609541Z",
     "shell.execute_reply.started": "2025-04-21T15:09:39.596138Z"
    },
    "id": "hhfMBve82oHi",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class smolModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # embedding layer which maps each token to a vector embedding\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.hidden_size\n",
    "        )\n",
    "\n",
    "        # Stack of decoder layers (LlamaDecoder) defined by the configuration\n",
    "        self.layers = nn.ModuleList([\n",
    "            LlamaDecoder(config) for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "\n",
    "        # RMSNorm: final layer normalization applied to hidden states\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=1e-05)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "        x = inputs_embeds\n",
    "\n",
    "        # Pass embeddings through each decoder layer\n",
    "        for i, decoder_layer in enumerate(self.layers):\n",
    "            layer_outputs = decoder_layer(\n",
    "                x,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            x = layer_outputs\n",
    "\n",
    "        # Final normalisation\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class smolLM(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the Language Model.\n",
    "    It passes the embeddings from the SmolLM backbone into a LM head.\n",
    "    The LM head generates logits over the space of the entire vocabulary for next word prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # SmolLM backbone which generates the contextualised embeddings for the input tokens\n",
    "        self.model = smolModel(config)\n",
    "        # The LM head which maps embeddings to logits over the vocabulary\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        # weights between LM head and the token_embedding layer are shared in the SmolLM architecture\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        # lm_head shares weights with the embedding layer\n",
    "        self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Input tokens are passed to the SmolLM backbone\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        # embeddings corresponding to each input token => (batch_size, seq_len, emb_dim)\n",
    "        x = outputs\n",
    "\n",
    "        # pass the embeddings through the LM head\n",
    "        logits = self.lm_head(x).float()\n",
    "        return {'logits': logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:39.611236Z",
     "iopub.status.busy": "2025-04-21T15:09:39.611001Z",
     "iopub.status.idle": "2025-04-21T15:09:39.624226Z",
     "shell.execute_reply": "2025-04-21T15:09:39.623649Z",
     "shell.execute_reply.started": "2025-04-21T15:09:39.611215Z"
    },
    "id": "SIoT13I92oHi",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def __generate(model, inputs, num_tokens, tokenizer, max_length=50):\n",
    "    \"\"\"\n",
    "    A basic greedy approach for text generation.\n",
    "    \"\"\"\n",
    "    collect = []\n",
    "    for _ in range(num_tokens):\n",
    "        output = model(**inputs)\n",
    "        output_id = torch.argmax(output['logits'][0, -1]).item()\n",
    "        collect.append(output_id)\n",
    "        if output_id == tokenizer.eos_token_id or len(collect) >= max_length:\n",
    "            break\n",
    "        # Update input_ids and attention_mask\n",
    "        new_token = torch.tensor([output_id], device=inputs['input_ids'].device)\n",
    "        inputs['input_ids'] = torch.cat([inputs['input_ids'][0], new_token]).unsqueeze(0)\n",
    "        inputs['attention_mask'] = F.pad(inputs['attention_mask'], (0, 1), value=1)\n",
    "    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(collect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:39.625130Z",
     "iopub.status.busy": "2025-04-21T15:09:39.624953Z",
     "iopub.status.idle": "2025-04-21T15:09:39.636379Z",
     "shell.execute_reply": "2025-04-21T15:09:39.635850Z",
     "shell.execute_reply.started": "2025-04-21T15:09:39.625117Z"
    },
    "id": "M4qgpRzi2oHi",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of a LoRA layer - a low-rank adaptation of a weight matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=16, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Initialize a LoRA layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # scaling factor\n",
    "        self.rank = rank\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "        # matrix A\n",
    "        self.A = nn.Parameter(torch.randn(rank, in_features) * 0.01)\n",
    "        nn.init.kaiming_normal_(self.A, a=math.sqrt(5)) #(lowest perplex = 5)\n",
    "        # nn.init.xavier_normal_(self.A) (lowest perplex = 9)\n",
    "        # nn.init.normal_(self.A, mean=0.0, std=0.02) (lowest preplex = 6)\n",
    "\n",
    "        # matrix B\n",
    "        self.B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        nn.init.zeros_(self.B)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## implement the forward pass here\n",
    "        resA = F.linear(x, self.A)\n",
    "        resB = F.linear(resA, self.B)\n",
    "        output = self.scaling * self.dropout(resB)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:39.637349Z",
     "iopub.status.busy": "2025-04-21T15:09:39.637129Z",
     "iopub.status.idle": "2025-04-21T15:09:39.652041Z",
     "shell.execute_reply": "2025-04-21T15:09:39.651471Z",
     "shell.execute_reply.started": "2025-04-21T15:09:39.637326Z"
    },
    "id": "Ys7OTCUn2oHj",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A linear layer with LoRA adaptation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, linear_layer, rank=8, alpha=16, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Initialize a LoRA-adapted linear layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # original linear layer\n",
    "        self.linear = linear_layer\n",
    "\n",
    "        # freeze the weights of the original layer\n",
    "        for p in self.linear.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # input and output dimensions from the linear layer\n",
    "        in_features = self.linear.in_features\n",
    "        out_features = self.linear.out_features\n",
    "\n",
    "        # create the LoRA adaptation layer\n",
    "        self.lora = LoRALayer(in_features, out_features, rank=rank, alpha=alpha, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## implement the forward pass here\n",
    "        output = self.linear(x) + self.lora(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:39.652764Z",
     "iopub.status.busy": "2025-04-21T15:09:39.652609Z",
     "iopub.status.idle": "2025-04-21T15:09:39.664188Z",
     "shell.execute_reply": "2025-04-21T15:09:39.663557Z",
     "shell.execute_reply.started": "2025-04-21T15:09:39.652752Z"
    },
    "id": "wFVn9FUK2oHj",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_lora_to_model(model, target_modules=None, rank=8, alpha=16, dropout=0.0):\n",
    "    \"\"\"\n",
    "    Add LoRA adapters to target modules in the model.\n",
    "\n",
    "    Returns:\n",
    "        Model with LoRA adapters\n",
    "    \"\"\"\n",
    "    ## your code here:\n",
    "    if target_modules is None:\n",
    "        target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'down_proj', 'gate_proj']\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for main_mod in model.modules():\n",
    "        sub_mod = main_mod._modules\n",
    "        citems = list(sub_mod.items())\n",
    "        for cname, cmod in citems:\n",
    "            is_linear = isinstance(cmod, nn.Linear)\n",
    "\n",
    "            check = False\n",
    "            for target in target_modules:\n",
    "                if target in cname:\n",
    "                    check = True\n",
    "                    break\n",
    "\n",
    "            if is_linear and check:\n",
    "                lora_wrapper = LoRALinear(linear_layer=cmod,rank=rank,alpha=alpha,dropout=dropout)\n",
    "                main_mod._modules[cname] = lora_wrapper\n",
    "\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, LoRALinear):\n",
    "            for p in module.lora.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    model_with_lora = model\n",
    "\n",
    "    return model_with_lora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBX7XhlC2oHj"
   },
   "source": [
    "**Initializing the Base and LoRA models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:39.665077Z",
     "iopub.status.busy": "2025-04-21T15:09:39.664846Z",
     "iopub.status.idle": "2025-04-21T15:09:41.764228Z",
     "shell.execute_reply": "2025-04-21T15:09:41.763693Z",
     "shell.execute_reply.started": "2025-04-21T15:09:39.665057Z"
    },
    "id": "B3q1te7t2oHj",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = smolConfig()\n",
    "base_model = smolLM(config)\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-135M\"\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "base_model.load_state_dict(reference_model.state_dict(), strict=False)\n",
    "\n",
    "target_modules = [\n",
    "    ## ADD MODULES HERE\n",
    "     'q_proj','k_proj', 'v_proj','o_proj','up_proj', 'down_proj', 'gate_proj'\n",
    "]\n",
    "\n",
    "## DO NOT CHANGE THIS\n",
    "lora_model = add_lora_to_model(\n",
    "    base_model,\n",
    "    target_modules=target_modules,\n",
    "    rank=4,\n",
    "    alpha=8,\n",
    "    dropout=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkqqkhWK2oHj"
   },
   "source": [
    "### Parameter Analysis\n",
    "\n",
    "Let's compare the parameter counts between the original model and the LoRA-enhanced version. Implement the parameter counting and analysis function.\n",
    "\n",
    "You should see that the % of trainable parameters in our `lora_model` should be <1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:41.765141Z",
     "iopub.status.busy": "2025-04-21T15:09:41.764877Z",
     "iopub.status.idle": "2025-04-21T15:09:41.781402Z",
     "shell.execute_reply": "2025-04-21T15:09:41.780671Z",
     "shell.execute_reply.started": "2025-04-21T15:09:41.765117Z"
    },
    "id": "mz8y9Xjt2oHk",
    "outputId": "9c7e996c-7dfc-4f11-d106-670caeee803d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters in Original Model: 135736128\n",
      "Trainable Parameters in LoRA Model: 1221120\n",
      "% of trainable parameters: 0.90%\n",
      "\n",
      "LoRA Parameters in each layer:\n",
      "q_proj: 138240\n",
      "k_proj: 92160\n",
      "v_proj: 92160\n",
      "o_proj: 138240\n",
      "up_proj: 253440\n",
      "down_proj: 253440\n",
      "gate_proj: 253440\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model: nn.Module, only_trainable: bool = False):\n",
    "    \"\"\"\n",
    "    Count the number of parameters in a model.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        only_trainable: If True, count only trainable parameters\n",
    "\n",
    "    Returns:\n",
    "        Number of parameters\n",
    "    \"\"\"\n",
    "    if only_trainable:\n",
    "            return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def analyze_parameters(original_model: nn.Module, lora_model: nn.Module):\n",
    "    \"\"\"\n",
    "    Analyze parameter counts between original and LoRA-adapted models.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with parameter statistics\n",
    "    \"\"\"\n",
    "\n",
    "    total_params = count_parameters(original_model)\n",
    "    trainable_params = count_parameters(lora_model, only_trainable=True)\n",
    "\n",
    "    # calculate parameter savings\n",
    "    param_percent = (trainable_params / total_params) * 100\n",
    "\n",
    "    # count parameters by layer type\n",
    "    lora_params_by_type = {}\n",
    "    for name, module in lora_model.named_modules():\n",
    "        if isinstance(module, LoRALayer):\n",
    "            # extract the module type from the name\n",
    "            parts = name.split(\".\")\n",
    "            module_type = next(\n",
    "                (\n",
    "                    p\n",
    "                    for p in parts\n",
    "                    if any(\n",
    "                        t in p\n",
    "                        for t in [\n",
    "                            ## ADD MODULES HERE\n",
    "                    'q_proj', 'k_proj', 'v_proj', 'o_proj','up_proj', 'down_proj', 'gate_proj'\n",
    "                        ]\n",
    "                    )\n",
    "                ),\n",
    "                \"other\",\n",
    "            )\n",
    "\n",
    "            # count parameters in this LoRA layer\n",
    "            params = sum(p.numel() for p in module.parameters())\n",
    "\n",
    "            # add to the count by type\n",
    "            if module_type in lora_params_by_type:\n",
    "                lora_params_by_type[module_type] += params\n",
    "            else:\n",
    "                lora_params_by_type[module_type] = params\n",
    "\n",
    "    stats =  {\n",
    "        \"total_params\": total_params,\n",
    "        \"trainable_params\": trainable_params,\n",
    "        \"param_percent\": param_percent,\n",
    "        \"params_by_type\": lora_params_by_type,\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "stats = analyze_parameters(base_model, lora_model)\n",
    "\n",
    "print(f\"Total Parameters in Original Model: {stats['total_params']}\")\n",
    "print(f\"Trainable Parameters in LoRA Model: {stats['trainable_params']}\")\n",
    "print(f\"% of trainable parameters: {stats['param_percent']:.2f}%\")\n",
    "print()\n",
    "print(f\"LoRA Parameters in each layer:\")\n",
    "for k, v in stats['params_by_type'].items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CeuqUvi2oHk"
   },
   "source": [
    "### Fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kM5dZjEH2oHk"
   },
   "source": [
    "#### Dataset Preparation\n",
    "Let's set up a small dataset for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:41.782416Z",
     "iopub.status.busy": "2025-04-21T15:09:41.782197Z",
     "iopub.status.idle": "2025-04-21T15:09:41.799595Z",
     "shell.execute_reply": "2025-04-21T15:09:41.799055Z",
     "shell.execute_reply.started": "2025-04-21T15:09:41.782400Z"
    },
    "id": "ZyfDDn8k2oHk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(\n",
    "    tokenizer,\n",
    "    dataset_name=\"databricks/databricks-dolly-15k\",\n",
    "    subset=None,\n",
    "    max_samples=500,\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare a dataset for fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Tokenizer to use\n",
    "        dataset_name: HuggingFace dataset name\n",
    "        subset: Dataset subset (if applicable)\n",
    "        max_samples: Maximum number of samples to use\n",
    "\n",
    "    Returns:\n",
    "        Processed dataset ready for training\n",
    "    \"\"\"\n",
    "    # load dataset\n",
    "    if subset:\n",
    "        dataset = load_dataset(dataset_name, subset)\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_name)\n",
    "\n",
    "    train_data = (\n",
    "        dataset[\"train\"]\n",
    "        .shuffle(seed=42)\n",
    "        .select(range(min(max_samples, len(dataset[\"train\"]))))\n",
    "    )\n",
    "\n",
    "    train_val_split = train_data.train_test_split(test_size=0.2, seed=42)\n",
    "    train_data = train_val_split[\"train\"]\n",
    "    val_data = train_val_split[\"test\"]\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        tokenized = tokenizer(examples[\"instruction\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        return tokenized\n",
    "\n",
    "    train_tokenized = train_data.map(\n",
    "        tokenize_function, batched=True, remove_columns=train_data.column_names\n",
    "    )\n",
    "    val_tokenized = val_data.map(\n",
    "        tokenize_function, batched=True, remove_columns=val_data.column_names\n",
    "    )\n",
    "\n",
    "    train_tokenized.set_format(\"torch\")\n",
    "    val_tokenized.set_format(\"torch\")\n",
    "\n",
    "    train_dataloader = DataLoader(train_tokenized, batch_size=8, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_tokenized, batch_size=8)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eewqwjc02oHl"
   },
   "source": [
    "#### Initialzing our Tokenizer and Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:41.803019Z",
     "iopub.status.busy": "2025-04-21T15:09:41.802630Z",
     "iopub.status.idle": "2025-04-21T15:09:44.212104Z",
     "shell.execute_reply": "2025-04-21T15:09:44.211569Z",
     "shell.execute_reply.started": "2025-04-21T15:09:41.803003Z"
    },
    "id": "LSYQ1he22oHl",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# the tokenizer does not have a defined padding token, so we initialize our own as the [EOS] token.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_dataloader, val_dataloader = prepare_dataset(tokenizer=tokenizer,  dataset_name=\"databricks/databricks-dolly-15k\", max_samples=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEH_tMgZ2oHl"
   },
   "source": [
    "**We can test our base model to ensure it's working correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:44.213106Z",
     "iopub.status.busy": "2025-04-21T15:09:44.212897Z",
     "iopub.status.idle": "2025-04-21T15:09:56.692149Z",
     "shell.execute_reply": "2025-04-21T15:09:56.691480Z",
     "shell.execute_reply.started": "2025-04-21T15:09:44.213091Z"
    },
    "id": "PEs5Q4-02oHl",
    "outputId": "bc3f5648-04fe-4038-f8b2-24a5386ac5cd",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Output generated====================\n",
      "The future of AI is  bright, but it’s not without its challenges. One of the biggest challenges is the lack of regulation and oversight. AI systems are often developed and deployed without the necessary safeguards in place to ensure they are safe and ethical. This lack of regulation can\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of AI is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "out = __generate(base_model, inputs, num_tokens=100, tokenizer=tokenizer)\n",
    "\n",
    "print('=='*10 + f' Output generated' + '=='*10)\n",
    "print(prompt + ' ' + out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l42Rid7y2oHl"
   },
   "source": [
    "#### Training Loop\n",
    "The training function for our model with LoRA adapters has been implemented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:56.693518Z",
     "iopub.status.busy": "2025-04-21T15:09:56.693019Z",
     "iopub.status.idle": "2025-04-21T15:09:56.705090Z",
     "shell.execute_reply": "2025-04-21T15:09:56.704400Z",
     "shell.execute_reply.started": "2025-04-21T15:09:56.693492Z"
    },
    "id": "0P_zjwRQ2oHl",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_lora(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    optimizer,\n",
    "    epochs=3,\n",
    "    device=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model with LoRA adapters.\n",
    "\n",
    "    Args:\n",
    "        model: LoRA-adapted model\n",
    "        train_dataloader: Training data\n",
    "        val_dataloader: Validation data\n",
    "        optimizer: PyTorch optimizer\n",
    "        epochs: Number of training epochs\n",
    "        device: Device to train on\n",
    "\n",
    "    Returns:\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_perplexity\": [],\n",
    "        \"val_perplexity\": [],\n",
    "    }\n",
    "\n",
    "    # add scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs[\"logits\"]\n",
    "\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            shift_attention_mask = attention_mask[:, 1:].contiguous()\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "\n",
    "            loss = loss.view(shift_labels.size())\n",
    "            loss = (loss * shift_attention_mask).sum() / shift_attention_mask.sum()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            progress_bar.set_postfix({\"train_loss\": loss.item()})\n",
    "\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "        avg_train_perplexity = torch.exp(torch.tensor(avg_train_loss)).item()\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "\n",
    "        progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs[\"logits\"]\n",
    "\n",
    "                shift_logits = logits[:, :-1, :].contiguous()\n",
    "                shift_labels = labels[:, 1:].contiguous()\n",
    "                shift_attention_mask = attention_mask[:, 1:].contiguous()\n",
    "\n",
    "                loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "                loss = loss_fct(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "                )\n",
    "\n",
    "                loss = loss.view(shift_labels.size())\n",
    "                loss = (loss * shift_attention_mask).sum() / shift_attention_mask.sum()\n",
    "\n",
    "                val_losses.append(loss.item())\n",
    "                progress_bar.set_postfix({\"val_loss\": loss.item()})\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        avg_val_perplexity = torch.exp(torch.tensor(avg_val_loss)).item()\n",
    "\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        history[\"val_loss\"].append(avg_val_loss)\n",
    "        history[\"train_perplexity\"].append(avg_train_perplexity)\n",
    "        history[\"val_perplexity\"].append(avg_val_perplexity)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs} - \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}, Train Perplexity: {avg_train_perplexity:.4f}, \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}, Val Perplexity: {avg_val_perplexity:.4f}\"\n",
    "        )\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eusRDraP2oHl"
   },
   "source": [
    "#### Training our Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "execution": {
     "iopub.execute_input": "2025-04-21T15:09:56.706159Z",
     "iopub.status.busy": "2025-04-21T15:09:56.705913Z",
     "iopub.status.idle": "2025-04-21T15:30:51.355241Z",
     "shell.execute_reply": "2025-04-21T15:30:51.354639Z",
     "shell.execute_reply.started": "2025-04-21T15:09:56.706137Z"
    },
    "id": "3Qe1PqTD2oHl",
    "outputId": "af934400-dbe3-4a62-8915-ccaf84374cff",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 300/300 [03:47<00:00,  1.32it/s, train_loss=3.65]\n",
      "Epoch 1/5 [Val]: 100%|██████████| 75/75 [00:22<00:00,  3.31it/s, val_loss=3.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 3.4935, Train Perplexity: 32.9011, Val Loss: 3.2364, Val Perplexity: 25.4413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 300/300 [03:48<00:00,  1.31it/s, train_loss=3.88]\n",
      "Epoch 2/5 [Val]: 100%|██████████| 75/75 [00:22<00:00,  3.30it/s, val_loss=3.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Train Loss: 3.2192, Train Perplexity: 25.0084, Val Loss: 3.1136, Val Perplexity: 22.5023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|██████████| 300/300 [03:48<00:00,  1.32it/s, train_loss=3.46]\n",
      "Epoch 3/5 [Val]: 100%|██████████| 75/75 [00:22<00:00,  3.31it/s, val_loss=3.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Train Loss: 3.0998, Train Perplexity: 22.1936, Val Loss: 3.0835, Val Perplexity: 21.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 300/300 [03:48<00:00,  1.32it/s, train_loss=2.8] \n",
      "Epoch 4/5 [Val]: 100%|██████████| 75/75 [00:22<00:00,  3.31it/s, val_loss=3.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Train Loss: 3.0436, Train Perplexity: 20.9797, Val Loss: 3.0657, Val Perplexity: 21.4494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Train]: 100%|██████████| 300/300 [03:48<00:00,  1.32it/s, train_loss=2.2] \n",
      "Epoch 5/5 [Val]: 100%|██████████| 75/75 [00:22<00:00,  3.31it/s, val_loss=3.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Train Loss: 2.9960, Train Perplexity: 20.0045, Val Loss: 3.0609, Val Perplexity: 21.3466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## DO NOT CHANGE THIS\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in lora_model.parameters() if p.requires_grad], lr=1e-4, weight_decay=0.01\n",
    ")\n",
    "\n",
    "history, trained_lora_model = train_lora(model=lora_model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, optimizer=optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-gcwVNm2oHl"
   },
   "source": [
    "**Optional: You can save your trained model in case you decide to do the assignment in parts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:30:51.356193Z",
     "iopub.status.busy": "2025-04-21T15:30:51.355979Z",
     "iopub.status.idle": "2025-04-21T15:30:52.616909Z",
     "shell.execute_reply": "2025-04-21T15:30:52.616064Z",
     "shell.execute_reply.started": "2025-04-21T15:30:51.356172Z"
    },
    "id": "UCszM4-q2oHl",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(trained_lora_model.state_dict(), \"lora_finetuned_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBro7vjt2oHl"
   },
   "source": [
    "#### Merging LoRA Weights for Inference\n",
    "For efficient inference, we can merge LoRA weights with the original weights.\n",
    "\n",
    "Implement the function `merge_lora_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:46:13.972593Z",
     "iopub.status.busy": "2025-04-21T15:46:13.972007Z",
     "iopub.status.idle": "2025-04-21T15:46:13.976996Z",
     "shell.execute_reply": "2025-04-21T15:46:13.976357Z",
     "shell.execute_reply.started": "2025-04-21T15:46:13.972572Z"
    },
    "id": "utvLIjRa2oHl",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def merge_lora_weights(model):\n",
    "    \"\"\"\n",
    "    Merge LoRA weights with original weights for efficient inference.\n",
    "\n",
    "    Args:\n",
    "        model: LoRA-adapted model\n",
    "\n",
    "    Returns:\n",
    "        Model with merged weights\n",
    "    \"\"\"\n",
    "    ## your code here\n",
    "    for name, module in list(model.named_modules()):\n",
    "        if isinstance(module, LoRALinear):\n",
    "            lora_A = module.lora.A\n",
    "            lora_B = module.lora.B\n",
    "            scaling = module.lora.scaling\n",
    "            delta = torch.matmul(lora_B, lora_A) * scaling\n",
    "            module.linear.weight.data.add_(delta.data)\n",
    "            # module.lora = None\n",
    "\n",
    "            # cleanup not necessary but replaced wtith nn.Linear\n",
    "            parent = model\n",
    "            path = name.split(\".\")\n",
    "            for p in path[:-1]:\n",
    "                parent = getattr(parent, p)\n",
    "            setattr(parent, path[-1], module.linear)\n",
    "\n",
    "    merged_model = model\n",
    "\n",
    "    return merged_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:46:16.374347Z",
     "iopub.status.busy": "2025-04-21T15:46:16.373752Z",
     "iopub.status.idle": "2025-04-21T15:46:16.378157Z",
     "shell.execute_reply": "2025-04-21T15:46:16.377522Z",
     "shell.execute_reply.started": "2025-04-21T15:46:16.374326Z"
    },
    "id": "XhSMNGhh2oHm",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge LoRA weights into the base model\n",
    "merged_model = merge_lora_weights(trained_lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UfJBDUs2oHr"
   },
   "source": [
    "**Optional: Save your merged model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:46:19.903350Z",
     "iopub.status.busy": "2025-04-21T15:46:19.902647Z",
     "iopub.status.idle": "2025-04-21T15:46:21.128951Z",
     "shell.execute_reply": "2025-04-21T15:46:21.128062Z",
     "shell.execute_reply.started": "2025-04-21T15:46:19.903326Z"
    },
    "id": "oqfq8nwO2oHr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(merged_model.state_dict(), \"merged_lora_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKG7KAOC2oHr"
   },
   "source": [
    "### Text Generation and Comparison\n",
    "Now let's compare text generation between models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqt_WEqQ2oHr"
   },
   "source": [
    "#### Loading in the fully finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:46:22.910265Z",
     "iopub.status.busy": "2025-04-21T15:46:22.909728Z",
     "iopub.status.idle": "2025-04-21T15:46:24.630854Z",
     "shell.execute_reply": "2025-04-21T15:46:24.630251Z",
     "shell.execute_reply.started": "2025-04-21T15:46:22.910243Z"
    },
    "id": "vQVXRTqB2oHr",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smolLM(\n",
       "  (model): smolModel(\n",
       "    (embed_tokens): Embedding(49152, 576)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoder(\n",
       "        (self_attn): GroupedQueryAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (rotary_emb): RotaryEmbedder()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (activation): SiLU()\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_base_model = smolLM(config)\n",
    "\n",
    "## add your model path here\n",
    "model_path = \"/kaggle/input/fullyfinetuned/pytorch/default/1/full_finetuned_smolLM.pth\"\n",
    "\n",
    "# Load the finetuned weights into the base model\n",
    "finetuned_base_model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "# Set to eval mode for inference\n",
    "finetuned_base_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D_YDZZI2oHr"
   },
   "source": [
    "#### We can now compare the fully finetuned and LoRA finetuned model to evaluate the effectiveness of using LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:46:28.655030Z",
     "iopub.status.busy": "2025-04-21T15:46:28.654560Z",
     "iopub.status.idle": "2025-04-21T15:46:28.664327Z",
     "shell.execute_reply": "2025-04-21T15:46:28.663508Z",
     "shell.execute_reply.started": "2025-04-21T15:46:28.655005Z"
    },
    "id": "cvcLGnrE2oHr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compare_generations(models, tokenizer, prompts, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Compare text generation between different model versions.\n",
    "\n",
    "    Args:\n",
    "        models: Dictionary of models to compare\n",
    "        tokenizer: Tokenizer\n",
    "        prompts: List of prompts to test\n",
    "        max_tokens: Maximum tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with generation results\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    results = []\n",
    "\n",
    "    def calculate_perplexity(model, inputs):\n",
    "        \"\"\"\n",
    "        Computes perplexity for a given model and input.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs[\"logits\"]\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = inputs[\"input_ids\"][:, 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "            perplexity = torch.exp(loss).item()\n",
    "        return perplexity\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        prompt_results = {\"Prompt\": prompt}\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "\n",
    "            start_time = time.time()\n",
    "            output = __generate(\n",
    "                model, inputs.copy(), num_tokens=max_tokens, tokenizer=tokenizer\n",
    "            )\n",
    "            end_time = time.time()\n",
    "\n",
    "            perplexity = calculate_perplexity(model, inputs)\n",
    "\n",
    "            prompt_results[f\"{model_name} Perplexity\"] = perplexity\n",
    "\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"Generated: {output}\")\n",
    "            print(f\"Time: {end_time - start_time:.2f}s\")\n",
    "            print(f\"Perplexity: {perplexity:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        results.append(prompt_results)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:46:32.655310Z",
     "iopub.status.busy": "2025-04-21T15:46:32.654994Z",
     "iopub.status.idle": "2025-04-21T15:46:45.624910Z",
     "shell.execute_reply": "2025-04-21T15:46:45.624121Z",
     "shell.execute_reply.started": "2025-04-21T15:46:32.655289Z"
    },
    "id": "reiyKHXZ2oHr",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Once upon a time, in a distant galaxy,\n",
      "Model: Fully Finetuned Model\n",
      "Generated:  there was a kind-hearted alien named Zork. One day, while exploring the universe, Zork came across a planet called Zorbaheim. Excited to explore, Zork decided to visit the planet and start his new life.\n",
      "\n",
      "\n",
      "Time: 2.50s\n",
      "Perplexity: 5.9674\n",
      "--------------------------------------------------\n",
      "Model: LoRA Finetuned Model\n",
      "Generated:  there was a spaceship named Zephyr who had just landed on a beautiful planet called Earth. Zephyr was excited to explore this new world but also felt a bit overwhelmed by its vastness and complexity. One day, while wandering around\n",
      "Time: 2.53s\n",
      "Perplexity: 5.9229\n",
      "--------------------------------------------------\n",
      "Prompt: The future of artificial intelligence is\n",
      "Model: Fully Finetuned Model\n",
      "Generated:  very bright.<|endoftext|>\n",
      "Time: 0.19s\n",
      "Perplexity: 22.8368\n",
      "--------------------------------------------------\n",
      "Model: LoRA Finetuned Model\n",
      "Generated:  uncertain. Will it be used for good or bad? Will it be used to help us or harm us? Will it be used to make us smarter or smarter? Will it be used to make us smarter or smarter? Will it be used to make\n",
      "Time: 2.46s\n",
      "Perplexity: 17.3586\n",
      "--------------------------------------------------\n",
      "Prompt: A wise old wizard once said,\n",
      "Model: Fully Finetuned Model\n",
      "Generated:  \"The more you learn, the better you get!\" And so, our tale continues, with dragons, griffins, minotaurs, centaurs, and half-elf half-human creatures learning valuable life lessons from their experiences.<|endoftext|>\n",
      "Time: 2.43s\n",
      "Perplexity: 53.2328\n",
      "--------------------------------------------------\n",
      "Model: LoRA Finetuned Model\n",
      "Generated:  \"The more you know, the more you can do.\" This quote reminds me of the importance of learning new things and expanding my knowledge. By doing so, I can better understand the world around me and make informed decisions.\n",
      "\n",
      "In conclusion,\n",
      "Time: 2.43s\n",
      "Perplexity: 20.4430\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define models for comparison\n",
    "models = {\n",
    "    \"Fully Finetuned Model\": finetuned_base_model,\n",
    "    \"LoRA Finetuned Model\": merged_model,\n",
    "}\n",
    "\n",
    "# Define prompts to test\n",
    "prompts = [\n",
    "    \"Once upon a time, in a distant galaxy,\",\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"A wise old wizard once said,\",\n",
    "]\n",
    "\n",
    "# Run the comparison\n",
    "df_results = compare_generations(models, tokenizer, prompts, max_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ww-2GHp2oHs"
   },
   "source": [
    "**Compare the perplexity scores of the models**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T15:31:09.135872Z",
     "iopub.status.busy": "2025-04-21T15:31:09.135580Z",
     "iopub.status.idle": "2025-04-21T15:31:09.140672Z",
     "shell.execute_reply": "2025-04-21T15:31:09.140115Z",
     "shell.execute_reply.started": "2025-04-21T15:31:09.135847Z"
    },
    "id": "DnShag4y2oHs",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════╤══════════════════════════════════════════╤════════════════════════════════════╤═══════════════════════════════════╕\n",
      "│    │ Prompt                                   │   Fully Finetuned Model Perplexity │   LoRA Finetuned Model Perplexity │\n",
      "╞════╪══════════════════════════════════════════╪════════════════════════════════════╪═══════════════════════════════════╡\n",
      "│  0 │ Once upon a time, in a distant galaxy,   │                            5.96741 │                           5.92286 │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────┼───────────────────────────────────┤\n",
      "│  1 │ The future of artificial intelligence is │                           22.8368  │                          17.3586  │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────┼───────────────────────────────────┤\n",
      "│  2 │ A wise old wizard once said,             │                           53.2328  │                          20.443   │\n",
      "╘════╧══════════════════════════════════════════╧════════════════════════════════════╧═══════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(df_results, headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmH2p5Fu2oHs"
   },
   "source": [
    "### Analysis and Discussion\n",
    "For this section, analyze your results and answer the following questions:\n",
    "\n",
    "**Question 2:** How does LoRA performance compare to full fine-tuning? What are the tradeoffs?\n",
    "\n",
    "**Ans:** LoRA achieves comparable to even superior performance with significantly reduced computational cost. As shown in the results, the LoRA-finetuned model outperformed the fully fine-tuned model on all evaluated prompts, producing lower perplexity scores, which indicate better predictions. The key tradeoff lies in efficiency: LORA only trains a small number of additional parameters, allowing faster training, less memory usage, and easier model storage and deployment. However, full fine-tuning might still be prefrable when complete control over the model’s behavior is required.\n",
    "\n",
    "**Question 3:** Which target modules benefit most from LoRA adaptation in SmolLM?\n",
    "\n",
    "**Ans:** Keeping in mind SmolLM, LoRA performs best when applied to the main projection layers of the attention which are the query, key,value, and output projections. These layers are responsible for computing the attention dynamics and have a high impact on the model’s ability to contextualize tokens effectively. This allows us to not modify the entire network but rather just apply LoRA to specific moduls.\n",
    "\n",
    "**Question 4:** How does rank value affect the quality of adaptation and the parameter count?\n",
    "\n",
    "**Ans:** The rank parameter in LoRA determines the dimensionality of the low-rank matrices used to approximate weight updates, directly impacting both the number of trainable parameters and the model’s capacity for adaptation. Higher ranks enable more expressive updates, improving performance on complex tasks but increasing computational cost/parameters. Lower ranks reduce memory and training overhead but may underfit with lesser parameters. Therefore, rank represents a tradeoff between adaptation quality and efficiency.\n",
    "\n",
    "**Question 5:** What are the practical benefits of LoRA for deploying fine-tuned models?\n",
    "\n",
    "**Ans:** Since LoRA freezes the base model and only trains a small number of low-rank adapter parameters, it drastically reduces storage and memory requirements (and by uqite a significant margin). This enables rapid deployment of multiple task-specific adapters without duplicating the entire model, which is especially valuable in resource-constrained environments. Additionally, LoRA adapters can be easily swapped, merged, or shared across systems, facilitating scalable and maintainable model serving. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ns1U3Ycc2oHs"
   },
   "source": [
    "# Fin."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 312429,
     "modelInstanceId": 291768,
     "sourceId": 349408,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
