{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA3 - LoRA implementation on SmolLM\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, you will learn how to integrate LoRA (Low-Rank Adapters) into the SmolLM model you implemented in PA2. Before starting working on this notebook, please make sure to go through the README.md provided as it will intoduce to the concepts relevant to the assignment.\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Follow along with the notebook, filling out the necessary code where instructed.\n",
    "\n",
    "- <span style=\"color: red;\">Read the Submission Instructions, Plagiarism Policy, and Late Days Policy in the attached PDF.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Do not remove any pre-written code.</span>\n",
    "\n",
    "- <span style=\"color: red;\">You must attempt all parts.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T08:03:38.391087Z",
     "iopub.status.busy": "2025-03-31T08:03:38.390796Z",
     "iopub.status.idle": "2025-03-31T08:03:38.395770Z",
     "shell.execute_reply": "2025-03-31T08:03:38.394925Z",
     "shell.execute_reply.started": "2025-03-31T08:03:38.391068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "# For tokenization and dataset loading\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# !pip install datasets\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing device here for future use if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:19.117587Z",
     "iopub.status.busy": "2025-03-31T07:51:19.117117Z",
     "iopub.status.idle": "2025-03-31T07:51:19.166963Z",
     "shell.execute_reply": "2025-03-31T07:51:19.165803Z",
     "shell.execute_reply.started": "2025-03-31T07:51:19.117564Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The implementation for the SmolLM model from PA2 has been added below for your convenience. You just need to run the next 5 cells in order to define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:22.501615Z",
     "iopub.status.busy": "2025-03-31T07:51:22.501295Z",
     "iopub.status.idle": "2025-03-31T07:51:22.505882Z",
     "shell.execute_reply": "2025-03-31T07:51:22.505153Z",
     "shell.execute_reply.started": "2025-03-31T07:51:22.501589Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class smolConfig:\n",
    "    vocab_size = 49152\n",
    "    hidden_size = 576\n",
    "    intermediate_size = 1536\n",
    "    num_hidden_layers = 30\n",
    "    num_heads = 9\n",
    "    kv_heads = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:23.959935Z",
     "iopub.status.busy": "2025-03-31T07:51:23.959547Z",
     "iopub.status.idle": "2025-03-31T07:51:23.973791Z",
     "shell.execute_reply": "2025-03-31T07:51:23.972948Z",
     "shell.execute_reply.started": "2025-03-31T07:51:23.959898Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"\n",
    "    Helper function to rotate the left half of a tensor along its final dimension.\n",
    "    \"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"\n",
    "    Applies RoPE on the query and key tensors.\n",
    "    \"\"\"\n",
    "    cos, sin = cos.to(q.device), sin.to(q.device)\n",
    "\n",
    "    # Unsqueexzing to enable broadcasting\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "    return q_embed, k_embed\n",
    "\n",
    "class RotaryEmbedder(nn.Module):\n",
    "    def __init__(self, dim, base):\n",
    "        super().__init__()\n",
    "        # Precompute frequency for sine/cosine embeddings\n",
    "        self.freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        # Generate positions (sequence indices) for the input\n",
    "        pos = torch.arange(x.shape[-2], dtype=torch.long)\n",
    "        # Compute angles for sine and cosine embeddings\n",
    "        angles = torch.einsum(\"p,f->pf\", pos.float(), self.freq).unsqueeze(dim=0)\n",
    "        # Duplicate angles for sine and cosine embeddings\n",
    "        emb = torch.cat((angles, angles), dim=-1)\n",
    "        # Return cosine and sine components of the positional embeddings\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Model dimensions and attention configurations\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.hidden_size // self.num_heads\n",
    "        self.kv_heads = config.kv_heads  # Number of key-value heads\n",
    "        self.rope_theta = 10000.0  # Scaling factor for rotary embeddings\n",
    "\n",
    "        # Linear projections for queries, keys, values, and output\n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "\n",
    "        # Rotary embedding generator\n",
    "        self.rotary_emb = RotaryEmbedder(base=self.rope_theta, dim=self.head_dim)\n",
    "\n",
    "    def _repeat_kv(self, x, n_rep):\n",
    "        batch, num_key_value_heads, slen, head_dim = x.shape\n",
    "        # Expand the number of key-value heads by repeating them\n",
    "        x = x[:, :, None, :, :].expand(\n",
    "            batch, num_key_value_heads, n_rep, slen, head_dim\n",
    "        )\n",
    "        # Reshape to align with the expected multi-head attention format\n",
    "        return x.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask=None):\n",
    "        # Input dimensions: (batch_size, seq_len, hidden_size)\n",
    "        b, q, _ = x.size()\n",
    "\n",
    "        # Project input hidden states into queries, keys, and values\n",
    "        q_states = self.q_proj(x)\n",
    "        k_states = self.k_proj(x)\n",
    "        v_states = self.v_proj(x)\n",
    "\n",
    "        # Reshape and transpose for multi-head attention\n",
    "        q_states = q_states.view(b, q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k_states = k_states.view(b, q, self.kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v_states = v_states.view(b, q, self.kv_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute rotary positional embeddings\n",
    "        cos, sin = self.rotary_emb(q_states)\n",
    "        cos = cos.to(q_states.device)\n",
    "        sin = sin.to(q_states.device)\n",
    "        # Apply positional embeddings to queries and keys\n",
    "        q_states, k_states = apply_rotary_pos_emb(q_states, k_states, cos, sin)\n",
    "\n",
    "        # Repeat key and value tensors to match the number of query heads\n",
    "        __kv_groups = self.num_heads // self.kv_heads\n",
    "        k_states = self._repeat_kv(k_states, __kv_groups)\n",
    "        v_states = self._repeat_kv(v_states, __kv_groups)\n",
    "\n",
    "        # Compute attention scores (scaled dot-product attention)\n",
    "        attn_weights = torch.matmul(q_states, k_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Add attention mask (e.g., for causal or padding masking)\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # Normalize attention weights using softmax\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights = nn.functional.dropout(attn_weights, 0)\n",
    "\n",
    "        # Compute attention output\n",
    "        attn_output = torch.matmul(attn_weights, v_states)\n",
    "        # Reshape and transpose back to original format\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(b, q, -1)\n",
    "\n",
    "        # Project the attention output back to the hidden size\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        # Return the final attention output\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:27.748636Z",
     "iopub.status.busy": "2025-03-31T07:51:27.748346Z",
     "iopub.status.idle": "2025-03-31T07:51:27.757156Z",
     "shell.execute_reply": "2025-03-31T07:51:27.756222Z",
     "shell.execute_reply.started": "2025-03-31T07:51:27.748615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        This is the Root Mean Square Normalisation class.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))  # Learnable scaling factor\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate variance along the last dimension (hidden size)\n",
    "        variance = x.pow(2).mean(-1, keepdim=True)\n",
    "\n",
    "        # Normalize and scale\n",
    "        x = x * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        \"\"\"\n",
    "        This is the gated MLP from the LLaMa architecture. Here we use the SiLU acitvation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.activation = nn.modules.activation.SiLU()\n",
    "\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.activation(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "\n",
    "class LlamaDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        This is the Llama decoder block.\n",
    "        \"\"\"\n",
    "        # Self Attention Module\n",
    "        self.self_attn = GroupedQueryAttention(config)\n",
    "\n",
    "        # FFN Module\n",
    "        self.mlp = MLP(hidden_size=config.hidden_size, intermediate_size=config.intermediate_size)\n",
    "\n",
    "        # Pre Attention and Post Attention normalisation\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, eps=1e-05)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=1e-05)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # Skip connection cache\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        # Pre-attention normalisation\n",
    "        x = self.input_layernorm(x)\n",
    "\n",
    "        # A causal attention mask (i.e., decoder can only look at tokens that it has generated thus far)\n",
    "        attention_mask = torch.triu(torch.full((attention_mask.shape[-1], attention_mask.shape[-1]),\n",
    "                                               fill_value=float('-inf')), diagonal=1)\n",
    "\n",
    "        attention_mask = attention_mask.to(x.device)\n",
    "\n",
    "        # Self-attention block\n",
    "        x = self.self_attn(x=x,attention_mask=attention_mask)\n",
    "        x += residual\n",
    "\n",
    "        # Skip connection cache for MLP\n",
    "        residual = x\n",
    "\n",
    "        # Pre-MLP normalisation\n",
    "        x = self.post_attention_layernorm(x)\n",
    "\n",
    "        # MLP block\n",
    "        x = self.mlp(x)\n",
    "        x += residual\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:30.099323Z",
     "iopub.status.busy": "2025-03-31T07:51:30.098923Z",
     "iopub.status.idle": "2025-03-31T07:51:30.106198Z",
     "shell.execute_reply": "2025-03-31T07:51:30.105377Z",
     "shell.execute_reply.started": "2025-03-31T07:51:30.099289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class smolModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # embedding layer which maps each token to a vector embedding\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.hidden_size\n",
    "        )\n",
    "\n",
    "        # Stack of decoder layers (LlamaDecoder) defined by the configuration\n",
    "        self.layers = nn.ModuleList([\n",
    "            LlamaDecoder(config) for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "\n",
    "        # RMSNorm: final layer normalization applied to hidden states\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=1e-05)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "        x = inputs_embeds\n",
    "\n",
    "        # Pass embeddings through each decoder layer\n",
    "        for i, decoder_layer in enumerate(self.layers):\n",
    "            layer_outputs = decoder_layer(\n",
    "                x,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            x = layer_outputs\n",
    "\n",
    "        # Final normalisation\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class smolLM(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the Language Model.\n",
    "    It passes the embeddings from the SmolLM backbone into a LM head.\n",
    "    The LM head generates logits over the space of the entire vocabulary for next word prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # SmolLM backbone which generates the contextualised embeddings for the input tokens\n",
    "        self.model = smolModel(config)\n",
    "        # The LM head which maps embeddings to logits over the vocabulary\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        # weights between LM head and the token_embedding layer are shared in the SmolLM architecture\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        # lm_head shares weights with the embedding layer\n",
    "        self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Input tokens are passed to the SmolLM backbone\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        # embeddings corresponding to each input token => (batch_size, seq_len, emb_dim)\n",
    "        x = outputs\n",
    "\n",
    "        # pass the embeddings through the LM head\n",
    "        logits = self.lm_head(x).float()\n",
    "        return {'logits': logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:33.910554Z",
     "iopub.status.busy": "2025-03-31T07:51:33.910266Z",
     "iopub.status.idle": "2025-03-31T07:51:33.915953Z",
     "shell.execute_reply": "2025-03-31T07:51:33.914906Z",
     "shell.execute_reply.started": "2025-03-31T07:51:33.910534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def __generate(model, inputs, num_tokens, tokenizer, max_length=50):\n",
    "    \"\"\"\n",
    "    A basic greedy approach for text generation.\n",
    "    \"\"\"\n",
    "    collect = []\n",
    "    for _ in range(num_tokens):\n",
    "        output = model(**inputs)\n",
    "        output_id = torch.argmax(output['logits'][0, -1]).item()\n",
    "        collect.append(output_id)\n",
    "        if output_id == tokenizer.eos_token_id or len(collect) >= max_length:\n",
    "            break\n",
    "        # Update input_ids and attention_mask\n",
    "        new_token = torch.tensor([output_id], device=inputs['input_ids'].device)\n",
    "        inputs['input_ids'] = torch.cat([inputs['input_ids'][0], new_token]).unsqueeze(0)\n",
    "        inputs['attention_mask'] = F.pad(inputs['attention_mask'], (0, 1), value=1)\n",
    "    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(collect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Problem\n",
    "\n",
    "In this assignment, we'll implement LoRA, which allows efficient fine-tuning by adding low-rank decomposition matrices to specific weight matrices in the model. For each targeted weight matrix $W \\in \\mathbb{R}^{d \\times k}$, we'll create two smaller matrices $A \\in \\mathbb{R}^{r \\times k}$ and $B \\in \\mathbb{R}^{d \\times r}$ where $r \\ll \\min(d, k)$.\n",
    "\n",
    "The key equation is:\n",
    "$W' = W + \\frac{\\alpha}{r}BA$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base LoRA Implementation\n",
    "First, we'll implement a generic LoRA module that can be applied to any linear layer in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:38.860139Z",
     "iopub.status.busy": "2025-03-31T07:51:38.859810Z",
     "iopub.status.idle": "2025-03-31T07:51:38.865906Z",
     "shell.execute_reply": "2025-03-31T07:51:38.865004Z",
     "shell.execute_reply.started": "2025-03-31T07:51:38.860110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of a LoRA layer - a low-rank adaptation of a weight matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=16, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Initialize a LoRA layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # scaling factor\n",
    "        self.rank = rank\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "        # matrix A\n",
    "        self.A = nn.Parameter(torch.zeros(rank, in_features)) # Initialize A\n",
    "\n",
    "        # matrix B\n",
    "        self.B = nn.Parameter(torch.zeros(out_features, rank)) # Initialize B with zeros\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## implement the forward pass here\n",
    "        # x_dropped = self.dropout(x)\n",
    "        # output = self.B(self.A(x_dropped))\n",
    "        # output = output * self.scaling\n",
    "        \n",
    "        # return output\n",
    "        result_A = F.linear(x, self.A)\n",
    "        result_B = F.linear(result_A, self.B)\n",
    "        output = self.scaling * self.dropout(result_B)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: Why is it important to initialize matrix B with zeros? How does this affect training at the beginning?\n",
    "\n",
    "**Ans**: Zero‑initializing the B matrix ensures that, at the very start of fine‑tuning, the LoRA adapters contribute nothing—so the frozen pretrained model’s behavior is preserved exactly. This  avoids sudden performance drops and keeps the loss stable. As training proceeds, the adapter gradually learns to adjust the weights, ensuring a smooth, parameter‑efficient adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA-Enhanced Linear Layer\n",
    "\n",
    "Now we'll create a wrapper for linear layers that incorporates LoRA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:42.216823Z",
     "iopub.status.busy": "2025-03-31T07:51:42.216505Z",
     "iopub.status.idle": "2025-03-31T07:51:42.222384Z",
     "shell.execute_reply": "2025-03-31T07:51:42.221600Z",
     "shell.execute_reply.started": "2025-03-31T07:51:42.216797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A linear layer with LoRA adaptation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, linear_layer, rank=8, alpha=16, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Initialize a LoRA-adapted linear layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # original linear layer\n",
    "        self.linear = linear_layer\n",
    "\n",
    "        # freeze the weights of the original layer\n",
    "        for p in self.linear.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # input and output dimensions from the linear layer\n",
    "        in_features = self.linear.in_features\n",
    "        out_features = self.linear.out_features\n",
    "\n",
    "        # create the LoRA adaptation layer\n",
    "        self.lora = LoRALayer(in_features, out_features, rank=rank, alpha=alpha, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## implement the forward pass here\n",
    "\n",
    "        output = self.linear(x) + self.lora(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying LoRA to SmolLM\n",
    "\n",
    "**LoRA Integration Strategy**\\\n",
    "We need to decide which weights in our model should be adapted with LoRA. In transformers, typical targets include:\n",
    "- Query, Key, Value projections in attention layers\n",
    "- Output projections in attention layers\n",
    "- Up/down projections in feed-forward networks\n",
    "\n",
    "Implement the function to add LoRA to specific linear layers in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:45.301595Z",
     "iopub.status.busy": "2025-03-31T07:51:45.301258Z",
     "iopub.status.idle": "2025-03-31T07:51:45.307586Z",
     "shell.execute_reply": "2025-03-31T07:51:45.306733Z",
     "shell.execute_reply.started": "2025-03-31T07:51:45.301566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_lora_to_model(model, target_modules=None, rank=8, alpha=16, dropout=0.0):\n",
    "    \"\"\"\n",
    "    Add LoRA adapters to target modules in the model.\n",
    "\n",
    "    Returns:\n",
    "        Model with LoRA adapters\n",
    "    \"\"\"\n",
    "    ## your code here:\n",
    "    if target_modules is None:\n",
    "        target_modules = [\"q_proj\", \"v_proj\",\"k_proj\"]\n",
    "\n",
    "    # freeze all\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # over every module\n",
    "    for parent_module in model.modules():\n",
    "        children = parent_module._modules  \n",
    "        child_items = list(children.items())\n",
    "        for child_name, child_module in child_items:\n",
    "            is_linear = isinstance(child_module, nn.Linear)\n",
    "            \n",
    "            name_matches = False\n",
    "            for target in target_modules:\n",
    "                if target in child_name:\n",
    "                    name_matches = True\n",
    "                    break\n",
    "\n",
    "            if is_linear and name_matches:\n",
    "                lora_wrapper = LoRALinear(linear_layer=child_module,rank=rank,alpha=alpha,dropout=dropout)\n",
    "                parent_module._modules[child_name] = lora_wrapper\n",
    "\n",
    "    # unfreeze only LoRA\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, LoRALinear):\n",
    "            for p in module.lora.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    model_with_lora = model\n",
    "    return model_with_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing the Base and LoRA models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:47.259792Z",
     "iopub.status.busy": "2025-03-31T07:51:47.259507Z",
     "iopub.status.idle": "2025-03-31T07:52:04.498219Z",
     "shell.execute_reply": "2025-03-31T07:52:04.497523Z",
     "shell.execute_reply.started": "2025-03-31T07:51:47.259771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = smolConfig()\n",
    "base_model = smolLM(config)\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-135M\"\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "base_model.load_state_dict(reference_model.state_dict(), strict=False)\n",
    "\n",
    "target_modules = [\n",
    "    ## ADD MODULES HERE\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "    \"k_proj\",\n",
    "    # \"v_proj\",\n",
    "    # \"o_proj\",\n",
    "    # \"up_proj\",\n",
    "    # \"down_proj\",\n",
    "]\n",
    "\n",
    "## DO NOT CHANGE THIS\n",
    "lora_model = add_lora_to_model(\n",
    "    base_model,\n",
    "    target_modules=target_modules,\n",
    "    rank=4,\n",
    "    alpha=8,\n",
    "    dropout=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Analysis\n",
    "\n",
    "Let's compare the parameter counts between the original model and the LoRA-enhanced version. Implement the parameter counting and analysis function.\n",
    "\n",
    "You should see that the % of trainable parameters in our `lora_model` should be <1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:52:09.233974Z",
     "iopub.status.busy": "2025-03-31T07:52:09.233692Z",
     "iopub.status.idle": "2025-03-31T07:52:09.251907Z",
     "shell.execute_reply": "2025-03-31T07:52:09.250906Z",
     "shell.execute_reply.started": "2025-03-31T07:52:09.233953Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters in Original Model: 134837568\n",
      "Trainable Parameters in LoRA Model: 322560\n",
      "% of trainable parameters: 0.24%\n",
      "\n",
      "LoRA Parameters in each layer:\n",
      "q_proj: 138240\n",
      "k_proj: 92160\n",
      "v_proj: 92160\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model: nn.Module, only_trainable: bool = False):\n",
    "    \"\"\"\n",
    "    Count the number of parameters in a model.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        only_trainable: If True, count only trainable parameters\n",
    "\n",
    "    Returns:\n",
    "        Number of parameters\n",
    "    \"\"\"\n",
    "    if only_trainable:\n",
    "            return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def analyze_parameters(original_model: nn.Module, lora_model: nn.Module):\n",
    "    \"\"\"\n",
    "    Analyze parameter counts between original and LoRA-adapted models.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with parameter statistics\n",
    "    \"\"\"\n",
    "\n",
    "    total_params = count_parameters(original_model)\n",
    "    trainable_params = count_parameters(lora_model, only_trainable=True)\n",
    "\n",
    "    # calculate parameter savings\n",
    "    param_percent = (trainable_params / total_params) * 100\n",
    "\n",
    "    # count parameters by layer type\n",
    "    lora_params_by_type = {}\n",
    "    for name, module in lora_model.named_modules():\n",
    "        if isinstance(module, LoRALayer):\n",
    "            # extract the module type from the name\n",
    "            parts = name.split(\".\")\n",
    "            module_type = next(\n",
    "                (\n",
    "                    p\n",
    "                    for p in parts\n",
    "                    if any(\n",
    "                        t in p\n",
    "                        for t in [\n",
    "                            ## ADD MODULES HERE\n",
    "                            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "                        ]\n",
    "                    )\n",
    "                ),\n",
    "                \"other\",\n",
    "            )\n",
    "\n",
    "            # count parameters in this LoRA layer\n",
    "            params = sum(p.numel() for p in module.parameters())\n",
    "\n",
    "            # add to the count by type\n",
    "            if module_type in lora_params_by_type:\n",
    "                lora_params_by_type[module_type] += params\n",
    "            else:\n",
    "                lora_params_by_type[module_type] = params\n",
    "\n",
    "    stats =  {\n",
    "        \"total_params\": total_params,\n",
    "        \"trainable_params\": trainable_params,\n",
    "        \"param_percent\": param_percent,\n",
    "        \"params_by_type\": lora_params_by_type,\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "stats = analyze_parameters(base_model, lora_model)\n",
    "\n",
    "print(f\"Total Parameters in Original Model: {stats['total_params']}\")\n",
    "print(f\"Trainable Parameters in LoRA Model: {stats['trainable_params']}\")\n",
    "print(f\"% of trainable parameters: {stats['param_percent']:.2f}%\")\n",
    "print()\n",
    "print(f\"LoRA Parameters in each layer:\")\n",
    "for k, v in stats['params_by_type'].items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preparation\n",
    "Let's set up a small dataset for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:52:15.362323Z",
     "iopub.status.busy": "2025-03-31T07:52:15.361994Z",
     "iopub.status.idle": "2025-03-31T07:52:15.368752Z",
     "shell.execute_reply": "2025-03-31T07:52:15.367859Z",
     "shell.execute_reply.started": "2025-03-31T07:52:15.362296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(\n",
    "    tokenizer,\n",
    "    dataset_name=\"databricks/databricks-dolly-15k\",\n",
    "    subset=None,\n",
    "    max_samples=500,\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare a dataset for fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Tokenizer to use\n",
    "        dataset_name: HuggingFace dataset name\n",
    "        subset: Dataset subset (if applicable)\n",
    "        max_samples: Maximum number of samples to use\n",
    "\n",
    "    Returns:\n",
    "        Processed dataset ready for training\n",
    "    \"\"\"\n",
    "    # load dataset\n",
    "    if subset:\n",
    "        dataset = load_dataset(dataset_name, subset)\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_name)\n",
    "\n",
    "    train_data = (\n",
    "        dataset[\"train\"]\n",
    "        .shuffle(seed=42)\n",
    "        .select(range(min(max_samples, len(dataset[\"train\"]))))\n",
    "    )\n",
    "\n",
    "    train_val_split = train_data.train_test_split(test_size=0.2, seed=42)\n",
    "    train_data = train_val_split[\"train\"]\n",
    "    val_data = train_val_split[\"test\"]\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        tokenized = tokenizer(examples[\"instruction\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        return tokenized\n",
    "\n",
    "    train_tokenized = train_data.map(\n",
    "        tokenize_function, batched=True, remove_columns=train_data.column_names\n",
    "    )\n",
    "    val_tokenized = val_data.map(\n",
    "        tokenize_function, batched=True, remove_columns=val_data.column_names\n",
    "    )\n",
    "\n",
    "    train_tokenized.set_format(\"torch\")\n",
    "    val_tokenized.set_format(\"torch\")\n",
    "\n",
    "    train_dataloader = DataLoader(train_tokenized, batch_size=8, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_tokenized, batch_size=8)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialzing our Tokenizer and Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:52:17.814073Z",
     "iopub.status.busy": "2025-03-31T07:52:17.813749Z",
     "iopub.status.idle": "2025-03-31T07:52:25.982440Z",
     "shell.execute_reply": "2025-03-31T07:52:25.981598Z",
     "shell.execute_reply.started": "2025-03-31T07:52:17.814046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# the tokenizer does not have a defined padding token, so we initialize our own as the [EOS] token.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_dataloader, val_dataloader = prepare_dataset(tokenizer=tokenizer,  dataset_name=\"databricks/databricks-dolly-15k\", max_samples=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can test our base model to ensure it's working correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:52:28.153861Z",
     "iopub.status.busy": "2025-03-31T07:52:28.153577Z",
     "iopub.status.idle": "2025-03-31T07:52:37.493377Z",
     "shell.execute_reply": "2025-03-31T07:52:37.492458Z",
     "shell.execute_reply.started": "2025-03-31T07:52:28.153840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Output generated====================\n",
      "The future of AI is  bright, but it’s not without its challenges. One of the biggest challenges is the lack of regulation and oversight. AI systems are often developed and deployed without the necessary safeguards in place to ensure they are safe and ethical. This lack of regulation can\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of AI is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "out = __generate(base_model, inputs, num_tokens=100, tokenizer=tokenizer)\n",
    "\n",
    "print('=='*10 + f' Output generated' + '=='*10)\n",
    "print(prompt + ' ' + out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop\n",
    "The training function for our model with LoRA adapters has been implemented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T08:05:58.185497Z",
     "iopub.status.busy": "2025-03-31T08:05:58.185212Z",
     "iopub.status.idle": "2025-03-31T08:05:58.197454Z",
     "shell.execute_reply": "2025-03-31T08:05:58.196618Z",
     "shell.execute_reply.started": "2025-03-31T08:05:58.185478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_lora(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    optimizer,\n",
    "    epochs=3,\n",
    "    device=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model with LoRA adapters.\n",
    "\n",
    "    Args:\n",
    "        model: LoRA-adapted model\n",
    "        train_dataloader: Training data\n",
    "        val_dataloader: Validation data\n",
    "        optimizer: PyTorch optimizer\n",
    "        epochs: Number of training epochs\n",
    "        device: Device to train on\n",
    "\n",
    "    Returns:\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_perplexity\": [],\n",
    "        \"val_perplexity\": [],\n",
    "    }\n",
    "\n",
    "    # add scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs[\"logits\"]\n",
    "\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            shift_attention_mask = attention_mask[:, 1:].contiguous()\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "\n",
    "            loss = loss.view(shift_labels.size())\n",
    "            loss = (loss * shift_attention_mask).sum() / shift_attention_mask.sum()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            progress_bar.set_postfix({\"train_loss\": loss.item()})\n",
    "\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "        avg_train_perplexity = torch.exp(torch.tensor(avg_train_loss)).item()\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "\n",
    "        progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs[\"logits\"]\n",
    "\n",
    "                shift_logits = logits[:, :-1, :].contiguous()\n",
    "                shift_labels = labels[:, 1:].contiguous()\n",
    "                shift_attention_mask = attention_mask[:, 1:].contiguous()\n",
    "\n",
    "                loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "                loss = loss_fct(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "                )\n",
    "\n",
    "                loss = loss.view(shift_labels.size())\n",
    "                loss = (loss * shift_attention_mask).sum() / shift_attention_mask.sum()\n",
    "\n",
    "                val_losses.append(loss.item())\n",
    "                progress_bar.set_postfix({\"val_loss\": loss.item()})\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        avg_val_perplexity = torch.exp(torch.tensor(avg_val_loss)).item()\n",
    "\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        history[\"val_loss\"].append(avg_val_loss)\n",
    "        history[\"train_perplexity\"].append(avg_train_perplexity)\n",
    "        history[\"val_perplexity\"].append(avg_val_perplexity)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs} - \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}, Train Perplexity: {avg_train_perplexity:.4f}, \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}, Val Perplexity: {avg_val_perplexity:.4f}\"\n",
    "        )\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training our Model. \n",
    "**DO NOT MODIFY THE HYPERPARAMETERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:29:00.870960Z",
     "iopub.status.busy": "2025-03-30T15:29:00.870670Z",
     "iopub.status.idle": "2025-03-30T15:49:59.277715Z",
     "shell.execute_reply": "2025-03-30T15:49:59.276749Z",
     "shell.execute_reply.started": "2025-03-30T15:29:00.870939Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]:  35%|███▍      | 104/300 [07:14<13:39,  4.18s/it, train_loss=3.56]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## DO NOT CHANGE THIS\u001b[39;00m\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\n\u001b[0;32m      4\u001b[0m     [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m lora_model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad], lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m history, trained_lora_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[132], line 64\u001b[0m, in \u001b[0;36mtrain_lora\u001b[1;34m(model, train_dataloader, val_dataloader, optimizer, epochs, device)\u001b[0m\n\u001b[0;32m     61\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mview(shift_labels\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m     62\u001b[0m loss \u001b[38;5;241m=\u001b[39m (loss \u001b[38;5;241m*\u001b[39m shift_attention_mask)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m shift_attention_mask\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m---> 64\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     67\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\Taha Faisal\\anaconda3\\envs\\Pytorch\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Taha Faisal\\anaconda3\\envs\\Pytorch\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Taha Faisal\\anaconda3\\envs\\Pytorch\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## DO NOT CHANGE THIS\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in lora_model.parameters() if p.requires_grad], lr=1e-4, weight_decay=0.01\n",
    ")\n",
    "\n",
    "history, trained_lora_model = train_lora(model=lora_model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, optimizer=optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional: You can save your trained model in case you decide to do the assignment in parts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:51:28.256504Z",
     "iopub.status.busy": "2025-03-30T15:51:28.256181Z",
     "iopub.status.idle": "2025-03-30T15:51:29.086334Z",
     "shell.execute_reply": "2025-03-30T15:51:29.085292Z",
     "shell.execute_reply.started": "2025-03-30T15:51:28.256478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(trained_lora_model.state_dict(), \"lora_finetuned_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging LoRA Weights for Inference\n",
    "For efficient inference, we can merge LoRA weights with the original weights.\n",
    "\n",
    "Implement the function `merge_lora_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:55:24.905198Z",
     "iopub.status.busy": "2025-03-31T07:55:24.904886Z",
     "iopub.status.idle": "2025-03-31T07:55:24.911480Z",
     "shell.execute_reply": "2025-03-31T07:55:24.910608Z",
     "shell.execute_reply.started": "2025-03-31T07:55:24.905156Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def merge_lora_weights(model):\n",
    "    \"\"\"\n",
    "    Merge LoRA weights with original weights for efficient inference.\n",
    "\n",
    "    Args:\n",
    "        model: LoRA-adapted model\n",
    "\n",
    "    Returns:\n",
    "        Model with merged weights\n",
    "    \"\"\"\n",
    "    ## your code here\n",
    "\n",
    "    merged_model = copy.deepcopy(model)\n",
    "\n",
    "    for name, module in merged_model.named_modules():\n",
    "        if isinstance(module, LoRALinear):\n",
    "            A = module.lora.A\n",
    "            B = module.lora.B\n",
    "            scaling = module.lora.scaling\n",
    "            delta_W = (B @ A) * scaling\n",
    "            module.weight.data += delta_W.data\n",
    "            module.lora = None\n",
    "\n",
    "    return merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:56:28.878718Z",
     "iopub.status.busy": "2025-03-31T07:56:28.878431Z",
     "iopub.status.idle": "2025-03-31T07:56:30.055127Z",
     "shell.execute_reply": "2025-03-31T07:56:30.054378Z",
     "shell.execute_reply.started": "2025-03-31T07:56:28.878695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge LoRA weights into the base model\n",
    "merged_model = merge_lora_weights(trained_lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional: Save your merged model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(merged_model.state_dict(), \"merged_lora_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation and Comparison\n",
    "Now let's compare text generation between models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in the fully finetuned model.\n",
    "Instead of having you fully finetune the model, we are sharing the weights to make your life a little easier. First, we'll load in our fully finetuned model. The weights are accesible through the drive link [Finetuned Base Model Weights](https://drive.google.com/drive/folders/1eIflNAp9UE4Fm8ZrBAzjDPsOCs-s_O55?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:56:39.645633Z",
     "iopub.status.busy": "2025-03-31T07:56:39.645337Z",
     "iopub.status.idle": "2025-03-31T07:56:41.845043Z",
     "shell.execute_reply": "2025-03-31T07:56:41.844187Z",
     "shell.execute_reply.started": "2025-03-31T07:56:39.645612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "finetuned_base_model = smolLM(config)\n",
    "\n",
    "## add your model path here\n",
    "model_path = \"\"\n",
    "\n",
    "# Load the finetuned weights into the base model\n",
    "finetuned_base_model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "# Set to eval mode for inference\n",
    "finetuned_base_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now compare the fully finetuned and LoRA finetuned model to evaluate the effectiveness of using LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:56:41.923160Z",
     "iopub.status.busy": "2025-03-31T07:56:41.922893Z",
     "iopub.status.idle": "2025-03-31T07:56:41.931922Z",
     "shell.execute_reply": "2025-03-31T07:56:41.930721Z",
     "shell.execute_reply.started": "2025-03-31T07:56:41.923139Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compare_generations(models, tokenizer, prompts, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Compare text generation between different model versions.\n",
    "\n",
    "    Args:\n",
    "        models: Dictionary of models to compare\n",
    "        tokenizer: Tokenizer\n",
    "        prompts: List of prompts to test\n",
    "        max_tokens: Maximum tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with generation results\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    results = []\n",
    "\n",
    "    def calculate_perplexity(model, inputs):\n",
    "        \"\"\"\n",
    "        Computes perplexity for a given model and input.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs[\"logits\"]\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = inputs[\"input_ids\"][:, 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "            perplexity = torch.exp(loss).item()\n",
    "        return perplexity\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        prompt_results = {\"Prompt\": prompt}\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "\n",
    "            start_time = time.time()\n",
    "            output = __generate(\n",
    "                model, inputs.copy(), num_tokens=max_tokens, tokenizer=tokenizer\n",
    "            )\n",
    "            end_time = time.time()\n",
    "\n",
    "            perplexity = calculate_perplexity(model, inputs)\n",
    "\n",
    "            prompt_results[f\"{model_name} Perplexity\"] = perplexity\n",
    "\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"Generated: {output}\")\n",
    "            print(f\"Time: {end_time - start_time:.2f}s\")\n",
    "            print(f\"Perplexity: {perplexity:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        results.append(prompt_results)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T07:56:49.703445Z",
     "iopub.status.busy": "2025-03-31T07:56:49.703109Z",
     "iopub.status.idle": "2025-03-31T07:57:04.313930Z",
     "shell.execute_reply": "2025-03-31T07:57:04.313155Z",
     "shell.execute_reply.started": "2025-03-31T07:56:49.703409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define models for comparison\n",
    "models = {\n",
    "    \"Fully Finetuned Model\": finetuned_base_model,\n",
    "    \"LoRA Finetuned Model\": merged_model,\n",
    "}\n",
    "\n",
    "# Define prompts to test\n",
    "prompts = [\n",
    "    \"Once upon a time, in a distant galaxy,\",\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"A wise old wizard once said,\",\n",
    "]\n",
    "\n",
    "# Run the comparison\n",
    "df_results = compare_generations(models, tokenizer, prompts, max_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the perplexity scores of the models**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(df_results, headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and Discussion\n",
    "For this section, analyze your results and answer the following questions:\n",
    "\n",
    "**Question 2:** How does LoRA performance compare to full fine-tuning? What are the tradeoffs?\n",
    "\n",
    "**Ans:** \n",
    "\n",
    "**Question 3:** Which target modules benefit most from LoRA adaptation in SmolLM?\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "**Question 4:** How does rank value affect the quality of adaptation and the parameter count?\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "**Question 5:** What are the practical benefits of LoRA for deploying fine-tuned models?\n",
    "\n",
    "**Ans:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 280292,
     "modelInstanceId": 259091,
     "sourceId": 303465,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
